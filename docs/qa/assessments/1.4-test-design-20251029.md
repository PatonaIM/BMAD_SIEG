# Test Design: Story 1.4 - OpenAI Integration & LangChain Setup

**Date:** 2025-10-29  
**Designer:** Quinn (Test Architect)  
**Story:** 1.4 - OpenAI Integration & LangChain Setup

## Test Strategy Overview

- **Total test scenarios:** 42
- **Unit tests:** 26 (62%)
- **Integration tests:** 14 (33%)
- **E2E tests:** 2 (5%)
- **Priority distribution:** P0: 18, P1: 16, P2: 8

### Test Level Rationale

This story focuses heavily on infrastructure setup with external API integration. The test pyramid shifts toward unit and integration tests because:

1. **High unit coverage** needed for provider abstraction, error handling, and cost calculation logic
2. **Integration tests** validate LangChain framework integration and database persistence
3. **Minimal E2E** appropriate since no user-facing endpoints yet (deferred to later stories)

---

## Test Scenarios by Acceptance Criteria

### AC1: OpenAI API key configured in environment variables with secure storage

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.4-UNIT-001 | Unit | P0 | Config loads OPENAI_API_KEY from environment | Security-critical: validates secrets management |
| 1.4-UNIT-002 | Unit | P0 | API key uses SecretStr type (not plaintext) | Security: prevents accidental key exposure in logs |
| 1.4-UNIT-003 | Unit | P0 | Config raises error if OPENAI_API_KEY missing | Fail-fast on misconfiguration |
| 1.4-UNIT-004 | Unit | P1 | Config loads all OpenAI settings with correct defaults | Validates configuration completeness |
| 1.4-INT-001 | Integration | P0 | Settings initialization succeeds with valid .env | End-to-end config validation |

**Risk Coverage:** Mitigates API key exposure (SEC-001), misconfiguration failures (CONF-001)

---

### AC2: LangChain library installed and configured for GPT-4 access

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.4-UNIT-005 | Unit | P1 | OpenAIProvider initializes with valid config | Tests provider setup logic |
| 1.4-UNIT-006 | Unit | P1 | Provider uses correct model from config | Validates model selection logic |
| 1.4-INT-002 | Integration | P0 | ChatOpenAI client initializes with LangChain | Critical: validates LangChain integration |
| 1.4-INT-003 | Integration | P1 | Provider factory returns correct provider type | Tests provider selection logic |
| 1.4-INT-004 | Integration | P2 | Provider supports both GPT-4 and GPT-4o-mini | Validates model flexibility |

**Risk Coverage:** Mitigates LangChain incompatibility (INT-001), model misconfiguration (CONF-002)

---

### AC3: Conversation memory implementation using LangChain's ConversationBufferMemory

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.4-UNIT-007 | Unit | P0 | ConversationMemoryManager initializes memory | Tests core memory setup |
| 1.4-UNIT-008 | Unit | P0 | save_context() stores message pair correctly | Validates memory persistence logic |
| 1.4-UNIT-009 | Unit | P0 | load_memory_variables() returns conversation history | Tests memory retrieval |
| 1.4-UNIT-010 | Unit | P0 | serialize_to_json() produces valid JSON | Critical for database storage |
| 1.4-UNIT-011 | Unit | P0 | deserialize_from_json() reconstructs memory | Critical for session recovery |
| 1.4-UNIT-012 | Unit | P1 | clear() removes all conversation history | Tests memory reset functionality |
| 1.4-UNIT-013 | Unit | P0 | Truncation keeps system prompt + last 5 messages | Prevents context length errors |
| 1.4-INT-005 | Integration | P0 | Memory persists to InterviewSession.conversation_memory JSONB | Database integration validation |
| 1.4-INT-006 | Integration | P0 | Memory deserializes from database correctly | Session recovery validation |
| 1.4-INT-007 | Integration | P1 | Multiple save_context calls maintain history | Tests conversation flow |

**Risk Coverage:** Mitigates data loss (DATA-001), context overflow (API-002), session recovery failures (SES-001)

---

### AC4: Prompt template system created for version-controlled interview prompts

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.4-UNIT-014 | Unit | P1 | PromptTemplateManager loads template from file | Tests file loading logic |
| 1.4-UNIT-015 | Unit | P1 | load_template() raises error for missing file | Validates error handling |
| 1.4-UNIT-016 | Unit | P1 | get_interview_prompt() combines system + role prompts | Tests prompt composition |
| 1.4-INT-008 | Integration | P1 | Load interview_system.txt successfully | Validates template file structure |
| 1.4-INT-009 | Integration | P2 | Template version header is parsed correctly | Tests version tracking |

**Risk Coverage:** Mitigates missing templates (FILE-001), prompt inconsistency (PROMPT-001)

---

### AC5: Basic prompt templates created for technical interview scenarios

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.4-INT-010 | Integration | P1 | Load React interview template | Validates role-specific template |
| 1.4-INT-011 | Integration | P1 | Load Python interview template | Validates role-specific template |
| 1.4-INT-012 | Integration | P1 | Load JavaScript interview template | Validates role-specific template |
| 1.4-INT-013 | Integration | P2 | All templates contain required placeholders | Validates template structure |

**Risk Coverage:** Mitigates incomplete templates (PROMPT-002), role mismatch (PROMPT-003)

---

### AC6: Token counting implemented to monitor API usage and costs

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.4-UNIT-017 | Unit | P0 | count_tokens() uses tiktoken for accuracy | Critical for cost tracking |
| 1.4-UNIT-018 | Unit | P0 | count_tokens_for_messages() sums message tokens | Validates conversation token counting |
| 1.4-UNIT-019 | Unit | P0 | estimate_cost() calculates GPT-4o-mini cost correctly | Financial accuracy critical |
| 1.4-UNIT-020 | Unit | P0 | estimate_cost() calculates GPT-4 cost correctly | Financial accuracy critical |
| 1.4-UNIT-021 | Unit | P1 | Token count matches OpenAI's calculation (±5%) | Validates tiktoken accuracy |
| 1.4-UNIT-022 | Unit | P1 | Cost estimation uses correct pricing per model | Prevents cost miscalculation |
| 1.4-INT-014 | Integration | P1 | Provider tracks tokens per API call | Validates token tracking integration |

**Risk Coverage:** Mitigates cost overrun (COST-001), incorrect billing (COST-002), budget monitoring (COST-003)

---

### AC7: Error handling for API rate limits and timeouts with graceful degradation

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.4-UNIT-023 | Unit | P0 | RateLimitError triggers exponential backoff | Critical reliability feature |
| 1.4-UNIT-024 | Unit | P0 | Max 3 retry attempts for rate limits | Prevents infinite retry loops |
| 1.4-UNIT-025 | Unit | P0 | APITimeoutError retries with 5s delay | Handles transient timeouts |
| 1.4-UNIT-026 | Unit | P0 | AuthenticationError does NOT retry | Security: fail fast on auth issues |
| 1.4-UNIT-027 | Unit | P0 | AuthenticationError raises custom exception | Enables proper error escalation |
| 1.4-UNIT-028 | Unit | P1 | APIError (500) retries up to 3 times | Handles server errors |
| 1.4-UNIT-029 | Unit | P0 | Context length error triggers truncation | Prevents API rejection |
| 1.4-UNIT-030 | Unit | P1 | Timeout set to 45 seconds | Validates timeout configuration |
| 1.4-UNIT-031 | Unit | P1 | Structured logging for all error types | Enables monitoring and debugging |
| 1.4-E2E-001 | E2E | P0 | Complete retry flow with mock rate limit responses | End-to-end resilience validation |

**Risk Coverage:** Mitigates API failures (API-001), rate limit exhaustion (API-003), unhandled exceptions (ERR-001)

---

### AC8: Local development supports OpenAI API mocking for cost-free testing

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.4-UNIT-032 | Unit | P1 | MockAIProvider returns predefined responses | Tests mock implementation |
| 1.4-UNIT-033 | Unit | P1 | MockAIProvider simulates realistic delays | Tests async behavior |
| 1.4-UNIT-034 | Unit | P1 | MockAIProvider supports role-specific responses | Tests response variation |
| 1.4-UNIT-035 | Unit | P0 | get_ai_provider() returns MockAIProvider when USE_MOCK_AI=true | Critical for dev environment |
| 1.4-UNIT-036 | Unit | P0 | get_ai_provider() returns OpenAIProvider when USE_MOCK_AI=false | Production configuration |
| 1.4-INT-015 | Integration | P1 | Full conversation flow with MockAIProvider | Cost-free integration testing |
| 1.4-E2E-002 | E2E | P2 | Switch between mock and real provider via config | Validates provider factory pattern |

**Risk Coverage:** Mitigates test costs (DEV-001), development friction (DEV-002), CI/CD pipeline costs (DEV-003)

---

## Cross-Cutting Test Scenarios

### Type Safety & Code Quality

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.4-UNIT-037 | Unit | P2 | All provider methods have type hints | Code quality standard |
| 1.4-UNIT-038 | Unit | P2 | All public functions have docstrings | Documentation standard |
| 1.4-UNIT-039 | Unit | P2 | Import organization follows coding standards | Maintainability |

### Performance

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.4-UNIT-040 | Unit | P2 | Token counting executes in <100ms | Performance baseline |
| 1.4-INT-016 | Integration | P2 | Memory serialization executes in <50ms | Database write performance |

### Security

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.4-UNIT-041 | Unit | P0 | API key never logged or exposed | Security audit requirement |
| 1.4-UNIT-042 | Unit | P0 | SecretStr redacts key in error messages | Prevents accidental exposure |

---

## Risk Coverage Matrix

| Risk ID | Risk Description | Test Coverage | Mitigation Level |
|---------|------------------|---------------|------------------|
| SEC-001 | API key exposure | 1.4-UNIT-001, 002, 041, 042 | High |
| CONF-001 | Configuration failures | 1.4-UNIT-003, 004, 1.4-INT-001 | High |
| INT-001 | LangChain incompatibility | 1.4-INT-002, 005, 006, 007 | High |
| DATA-001 | Memory data loss | 1.4-UNIT-010, 011, 1.4-INT-005, 006 | High |
| API-001 | OpenAI API failures | 1.4-UNIT-023-031, 1.4-E2E-001 | High |
| API-002 | Context length exceeded | 1.4-UNIT-013, 029 | Medium |
| COST-001 | Unexpected cost overrun | 1.4-UNIT-017-022, 1.4-INT-014 | High |
| DEV-001 | High test costs | 1.4-UNIT-032-036, 1.4-INT-015 | High |

---

## Recommended Execution Order

### Phase 1: Fast Fail (Critical Infrastructure)
1. **1.4-UNIT-001 to 003** - Config and security basics
2. **1.4-UNIT-005 to 007** - Provider initialization
3. **1.4-INT-001 to 003** - LangChain integration

### Phase 2: Core Functionality
4. **1.4-UNIT-008 to 013** - Memory management
5. **1.4-UNIT-017 to 022** - Token counting and costs
6. **1.4-UNIT-023 to 031** - Error handling

### Phase 3: Integration Validation
7. **1.4-INT-005 to 007** - Database persistence
8. **1.4-INT-008 to 013** - Prompt templates
9. **1.4-INT-014 to 016** - Provider integration

### Phase 4: Development Tools
10. **1.4-UNIT-032 to 036** - Mock provider
11. **1.4-INT-015** - Mock integration flow

### Phase 5: End-to-End & Quality
12. **1.4-E2E-001 to 002** - Complete workflows
13. **1.4-UNIT-037 to 042** - Code quality & security

---

## Test Implementation Notes

### Unit Test Mocking Strategy

**Mock all external dependencies:**
- OpenAI API calls → Use `pytest-mock` with fixture responses
- File system → Mock `open()` for prompt template loading
- Environment variables → Use `monkeypatch` to set test values
- Database → Not needed at unit level

### Integration Test Setup

**Use real components where possible:**
- Real `ConversationBufferMemory` from LangChain
- Real test database with SQLAlchemy
- Real prompt template files (test fixtures)
- Mock only OpenAI API (use `MockAIProvider`)

### Test Data Fixtures

**Create reusable fixtures in `conftest.py`:**
```python
@pytest.fixture
def mock_openai_response():
    return {
        "id": "chatcmpl-test123",
        "choices": [{"message": {"content": "Test response"}}],
        "usage": {"prompt_tokens": 56, "completion_tokens": 31}
    }

@pytest.fixture
def sample_conversation_memory():
    return {
        "messages": [
            {"role": "system", "content": "You are an AI interviewer"},
            {"role": "assistant", "content": "Hello"},
            {"role": "user", "content": "Hi"}
        ]
    }
```

### Coverage Goals

- **Overall target:** 80%+ coverage
- **Provider modules:** 90%+ (critical infrastructure)
- **Utility modules:** 85%+ (used widely)
- **Integration paths:** 75%+ (acceptable for DB/API tests)

---

## Test Deliverables

### Test Files to Create

**Unit Tests:**
- `backend/tests/unit/test_openai_provider.py` (Tests: 1.4-UNIT-005, 006, 017-031, 041, 042)
- `backend/tests/unit/test_conversation_memory.py` (Tests: 1.4-UNIT-007-013)
- `backend/tests/unit/test_prompt_loader.py` (Tests: 1.4-UNIT-014-016)
- `backend/tests/unit/test_token_counter.py` (Tests: 1.4-UNIT-017-022)
- `backend/tests/unit/test_mock_ai_provider.py` (Tests: 1.4-UNIT-032-036)
- `backend/tests/unit/test_config.py` (Tests: 1.4-UNIT-001-004, 037-040)

**Integration Tests:**
- `backend/tests/integration/test_langchain_integration.py` (Tests: 1.4-INT-002-007, 014-016)
- `backend/tests/integration/test_prompt_templates.py` (Tests: 1.4-INT-008-013)
- `backend/tests/integration/test_config_integration.py` (Tests: 1.4-INT-001)

**E2E Tests:**
- `backend/tests/e2e/test_provider_resilience.py` (Tests: 1.4-E2E-001, 002)

### Test Data Files

- `backend/tests/fixtures/prompts/test_system.txt`
- `backend/tests/fixtures/prompts/test_react.txt`
- `backend/tests/fixtures/openai_responses.json`

---

## Quality Gate Impact

This test design contributes to quality gate decision:

**✅ PASS Criteria:**
- All P0 tests passing (18 tests)
- Coverage ≥80% for provider/memory modules
- All 8 acceptance criteria have test coverage
- No security tests failing

**⚠️ CONCERNS Criteria:**
- 1-2 P0 tests failing with known workarounds
- Coverage 70-79%
- Missing tests for 1 AC (with justification)

**❌ FAIL Criteria:**
- 3+ P0 tests failing
- Coverage <70%
- Security tests failing (UNIT-001, 002, 041, 042)
- No test coverage for critical ACs (1, 3, 6, 7)

---

## Dependencies for Test Execution

**Python packages needed:**
```
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-mock>=3.12.0
pytest-cov>=4.1.0
```

**Test environment setup:**
- Test database initialized (from conftest.py)
- `.env.test` file with mock configuration
- Prompt template fixtures in place

---

## Long-term Test Maintenance

**Monthly review triggers:**
- LangChain version updates → Revalidate INT-002, 005-007
- OpenAI API changes → Update mock responses, revalidate error handling
- Cost model changes → Update UNIT-017-022 with new pricing

**Test stability monitoring:**
- Track flaky tests (timeouts, race conditions)
- Monitor test execution time (goal: <5min total)
- Review and update mock data quarterly

---

**Test Design Completed:** 2025-10-29  
**Next Steps:** Execute test implementation alongside development, use for quality gate assessment
